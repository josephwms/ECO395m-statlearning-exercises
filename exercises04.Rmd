---
title: 'ECO 395M: StateLearning Exercise 4'
author: "Aahil Navroz, Joseph Williams, Qi Suqian"
date: "`r Sys.Date()`"
output: 
  html_document:
  md_document:
---
# ECO 395M: StatLearning Exercise 3

Aahil Navroz, Joseph Williams, Qi Suqian

04/22/2023

## Clustering and PCA
We first did a preliminary analysis to these chemical properties. We plot the correlation plot of them with color and quality. According to the plot, total.sulfur.dioxide, volatile.acidity and chlorides are the 3 most related variables to color and alcohol is the most related variable to quality
```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(ggcorrplot)
library(tidyverse)
library(ClusterR)
library(arules)
library(arulesViz)
library(rgl)

wine = read.csv("./data/wine.csv")
chemical = wine[,-(12:13)]
chemical = scale(chemical, center=TRUE, scale=TRUE)
color_sample = wine[,-12]
color_sample$color = as.numeric(color_sample$color == "red")
quality_sample = wine[,-13]
ggcorrplot(cor(color_sample),lab = TRUE,hc.order = TRUE)
ggcorrplot(cor(quality_sample),lab = TRUE,hc.order = TRUE)
```
We implemented the PCA algorithm on the 11 chemical properties and showed the PCA variance plot. We choose to decrease the data dimension to 4, which means we choose the first 4 PcA components. We then showed the unit vector of these 4 PCA components.
```{r, message=FALSE, echo=FALSE, warning=FALSE}
# pca results
PCApilot = prcomp(chemical, scale=TRUE, rank=4)
plot(PCApilot)
summary(PCApilot)
#the variance decrease slowly after PC4,so we choose to reduce the data dimension to 4
round(PCApilot$rotation[,1:4],2) 
```
Now we plot the wine color data to PC1 and PC2. From all the combination of PC components, PC1 and PC2 best distinguish the red wine to white wine.
```{r, message=FALSE, echo=FALSE, warning=FALSE}
color_pca = data.frame(wine$color, PCApilot$x[,1:4])
ggplot(color_pca, aes(x = PC1, y = PC2, color = wine.color)) + 
  geom_point(alpha = 0.5) +
  labs(title = "PCA of Wine Chemical Properties",
       x = "Principal Component1",
       y = "Principal Componen2",
       color = "Wine Speices") +
  theme_minimal() + 
  theme(legend.position = "right")
```
Then we try thw K-means cluster algorithm. We choose to cluster the data into two subsets and constructed the confusion matrix and this the accuracy, which reaches 98.58%. So we think both these 2 methods greatly distinguish the wine color data.
```{r, message=FALSE, echo=FALSE, warning=FALSE}
Kmeans_clust_col = KMeans_rcpp(chemical, clusters=2, num_init=25)
col_cluster = Kmeans_clust_col$cluster
col_cluster = as.factor(col_cluster)
conf_matrix_col = table(wine$color, col_cluster)
dimnames(conf_matrix_col) = list(True_Color = c("red","white"), Cluster_Label = c("red","white"))
print(conf_matrix_col)
accuracy_col = sum(diag(conf_matrix_col)) / sum(conf_matrix_col)
print(accuracy_col)
```
Then we tried to use PCA to distinguish the wine quality. The plot shows that PCA method doesn't perform well and it's hard to manually distinguish them, so we also made a 3D plot for better visualization. Both plots showed that PCA perform bad for wine quality.
```{r, message=FALSE, echo=FALSE, warning=FALSE}
# pca for quality
quality_pca = data.frame(wine$quality, PCApilot$x[,1:4])
quality_pca$wine.quality = as.factor(quality_pca$wine.quality)
ggplot(quality_pca, aes(x = PC1, y = PC2, color = wine.quality)) + 
  geom_point(alpha = 0.5) +
  labs(title = "PCA of Wine Chemical Properties",
       x = "Principal Component1",
       y = "Principal Componen2",
       color = "Wine Quality") +
  theme_minimal() + 
  theme(legend.position = "right")
open3d()
plot3d(quality_pca$PC1, quality_pca$PC2, quality_pca$wine.quality, size=3,col=quality_pca$wine.quality)
```
Now we check the performance of K-means method for wine quality. The accuracy is only 15%, which is also very bad. As a result, both methods seem not capable of distinguishing the higher from the lower quality wines.
```{r, message=FALSE, echo=FALSE, warning=FALSE}
# k-means for quality
Kmeans_clust_qua = KMeans_rcpp(chemical, clusters=7, num_init=25)
qua_cluster = Kmeans_clust_qua$cluster
qua_cluster = as.factor(qua_cluster)
conf_matrix_qua = table(wine$quality, qua_cluster)
dimnames(conf_matrix_qua) = list(True_Color = c(3,4,5,6,7,8,9),Cluster_Label = c(3,4,5,6,7,8,9))
print(conf_matrix_qua)
accuracy_qua = sum(diag(conf_matrix_qua)) / sum(conf_matrix_qua)
print(accuracy_qua)
```

## Market segmentation

*Use the data to come up with some interesting, well-supported insights about the audience and give your client some insight as to how they might position their brand to maximally appeal to each market segment.*

To get a basic idea of our data lets start with a two-way correlation plot.

```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(dplyr)

market = read.csv("./data/social_marketing.csv")
features = market %>% select(-X)
ggcorrplot(cor(features),
           lab = TRUE,
           hc.order = TRUE,
           lab_size = .1,
           tl.cex=8
           )      # Vertical alignment

```

Great! So already we're seeing some clusters.  I notice see two way correlation within the following groups: 

* group1_familyvalues: `parenting`, `religion`, `sports_fandom`, `food`, `school`, `family`
* group2_collegeboy: `college_uni`, `online_gaming`, `sports_playing`
* group3_fashionable: `beauty`, `cooking`, `fashion`
* group4_yuppie: `personal_fitness`, `health_nutrition`, `outdoors`
* group5_neoliberal: `politics`, `travel`, `computers`
* group6_socialyte: `shopping`, `chatter`, `photo_sharing`

Lets organize counts to see how many followers had at least two or more tweets in at least 2, (or 3 for family_values and socialyte) variables of each cluster.  Lets also filter out users who are in more than 3 of these respective groups to eliminate generalists and get a better personality portrait of our followers.

```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(ggplot2)

market1 <- market %>% 
  rowwise() %>%
  mutate(
    group1_familyvalues = as.numeric(sum(parenting > 1, religion > 1, sports_fandom > 1, food > 1, school > 1, family > 1) >= 3),
    group2_collegeboy = as.numeric(sum(college_uni > 1, online_gaming > 1, sports_playing > 1) >= 2),
    group3_fashionable = as.numeric(sum(beauty > 1, cooking > 1, fashion > 1) >= 2),
    group4_yuppie = as.numeric(sum(personal_fitness > 1, health_nutrition > 1, outdoors > 1) >= 2),
    group5_neoliberal = as.numeric(sum(politics > 1, travel > 1, computers > 1) >= 2),
    group6_socialyte = as.numeric(sum(shopping > 1, chatter > 1, photo_sharing > 1) >= 3),
    num_groups = sum(group1_familyvalues,group2_collegeboy,group3_fashionable,group4_yuppie,group5_neoliberal,group6_socialyte)
  ) %>%
  ungroup()

group_counts1 <- market1 %>%
  filter(num_groups<4) %>%
  summarise(
    group1_familyvalues = sum(group1_familyvalues, na.rm = TRUE),
    group2_collegeboy = sum(group2_collegeboy, na.rm = TRUE),
    group3_fashionable = sum(group3_fashionable, na.rm = TRUE),
    group4_yuppie = sum(group4_yuppie, na.rm = TRUE),
    group5_neoliberal = sum(group5_neoliberal, na.rm = TRUE),
    group6_socialyte = sum(group6_socialyte, na.rm = TRUE)
  ) %>%
  pivot_longer(
    cols = everything(),
    names_to = "group",
    values_to = "user_count"
  )

print(group_counts1)

# Create the bar plot
plot1 = ggplot(group_counts1, aes(x = group, y = user_count)) +
  geom_bar(stat = "identity") +
  ggtitle("Counts by Group for Users in Max 3 Groups") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Over 1500 users are in the loosely constructed `familyvalues', 'yuppie', and 'socialyte' groups, respectively.  Even with filtering efforts, however, many users are likely counted 2 or even 3 times. Before we go farther in this direction, lets shift to machine learning algorithms so that our clusters are definite and exhaustive.  We'll start with K-means and K_means++ clustering with K=5-7 clusters, since we see 6 key groups off the bat.  See below visualizations and counts per cluster.


```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(ClusterR)  # for kmeans++
library(ggrepel)

#for K++
# Extract and round centroids
# centroids <- clust2$centroids
# colnames(centroids) <- colnames(ft)
# clust2_rounded <- round(centroids, 2)
# print(clust2_rounded)

# Data preprocessing
ft = market[,-1]  # Exclude the first column
ft = scale(ft, center=TRUE, scale=TRUE)

# Execute kmeans
set.seed(1)
k_values <- c(5, 7, 6)

for (k in k_values) {
  clust = kmeans(ft, k, nstart=50)
  
  # Extract and round centroids
  centroids <- round(clust$centers, 2)
  
  #return cluster data for k=6
  # if (k==6){
  #   print(centroids)
  # }
    
  
  # Convert centroids to a dataframe
  centroids_df <- as.data.frame(centroids)
  centroids_df <- centroids_df %>%
    rownames_to_column(var = "centroid")
  
  # Convert centroids dataframe to long format for plotting
  centroids_long <- centroids_df %>%
    gather(key = "variable", value = "value", -centroid)
  
  # Plot pie charts for each centroid
  pie_plots <- ggplot(centroids_long, aes(x = "", y = value, fill = variable)) +
    geom_bar(stat = "identity", width = 1) +
    coord_polar("y", start=0) +
    facet_wrap(~centroid, ncol = 3) +
    labs(title = paste("Pie Charts of Centroids, K =", k),
         fill = "Variable") +
    theme_void()
  
  # Add labels for variables with centroid value > 1 without overlap
  label_data <- centroids_long %>%
    filter(value > 1)
  
  pie_plots <- pie_plots +
    geom_text_repel(data = label_data, aes(label = variable, y = value), 
                    position = position_stack(vjust = 0.5), 
                    size = 3) +
    theme(legend.position = "none")
  
  # Count number of users in each cluster
  cluster_counts <- as.data.frame(table(clust$cluster))
  colnames(cluster_counts) <- c("Cluster", "Users")
  
  
  print(pie_plots)
  print(cluster_counts)
  
}

```

We are able to confirm using unsupervised learning pretty much the same clusters we identified using intuition and basic tools. Labeled variables have centroid values over a certain, relatively-high threshold.  Our results use standard K-means but we've verified that results are repeatable with K-means++ start up.

Altogether, its clear that K=6 weeds out the most spam posts and maintains an even distribution between the other categories, a key requirement of clustering.


#### Insights & Recommended Steps
We've identified 5 roughly even-sized market segments using K-means clustering with K=6.  Here are the two largest in descending order:

- **Health-conscious adults (likely mid-twenties to thirties)**. In my city we call these yoga enthusiasts and REI shoppers 'yuppies', short for 'young professionals'.  This market segment shared twitter engagement in `health_nutrition`, `personal_fitness`, `outdoors`.  This segment also showed notable interest for `eco`, though it did not meet the display threshold.  We recommend an approach that shows sustainability efforts, and connects your product to outdoor engagement and mental health.

- **Traditional Americans**. Don't forget about the heartland, the silent majority, your minivan moms and sports bar dads. This market segment showed over-threshold engagement with more categories than any other group, by far, indicating they are 'classic American' consumers- not part of any niche group.  Key characteristics align with traditional values, and include `family`, `food`, `religion`, `sports fandom`, `school` and `parenting`. To appeal to this group, show that your product could easily find its way to a children's soccer game or family reunion.

Overall, to appeal to both groups and maximize market outreach, perhaps you are the beverage of choice for the modern parent... but not *too* modern. Perhaps its being consumed on a good ol' fashioned camping trip. Don't forget to put ice in the cooler!






## Association rules for grocery purchases

*Find some interesting association rules for these shopping baskets. Pick your own thresholds for lift and confidence; just be clear what these thresholds are and how you picked them. Do your discovered item sets make sense? Present your discoveries using an interesting visualization or two, along with no more than one page of typed text.*


We'll begin with some initial data wrangling in order to view the data.  See below head() for first 5 baskets.

```{r, message=FALSE, echo=FALSE, warning=FALSE}



# First we need to transform the text file into a readable format to determine association rules for shopping baskets. 

# Read the data from the file
groceries_raw = readLines("./data/groceries.txt")

# Split each line into items, remove duplicates, and create a list
groceries_list = lapply(groceries_raw, function(line) unique(strsplit(line, ",")[[1]]))

# Convert the list to transaction class
groceries = as(groceries_list, "transactions")

# Preview the data
inspect(head(groceries, n = 5))

```


Next, we'll initialize association analysis with very low support and confidence levels and tune from there depending on distributions we see when plotting the rules. 

Based on spread of confidence and lift, we tweaked 'supp', 'conf' and 'minlen' parameters to find a sweet spot where confidence is not too restrictive but there is enough variation in lift to identify the most significant associations. Eventually, we arrived at parameter values of 0.01 for support and 0.25 for confidence.


```{r, message=FALSE, echo=FALSE, warning=FALSE, results='hide'}
rules = apriori(groceries, parameter = list(supp = 0.01, conf = 0.25, minlen = 2))

```


```{r, message=FALSE, echo=FALSE, warning=FALSE}

plot(rules, jitter = 0)

```

170 is quite a large number for rules in this case. Lets only focus on lifts that are greater than 2.5 in order to analyze only significant associations.

```{r, message=FALSE, echo=FALSE, warning=FALSE, options(width = 80)}
rules = subset(rules, lift > 2.5)

 # Round the specified columns
rules@quality$support <- round(rules@quality$support, 3)
rules@quality$confidence <- round(rules@quality$confidence, 3)
rules@quality$coverage <- round(rules@quality$coverage, 3)
rules@quality$count <- round(rules@quality$count, 3)
rules@quality$lift <- round(rules@quality$lift, 3)
 
options(width = 140)
inspect(sort(rules, by = "lift"))


```

Looking at the rules sorted by lift, many of these item sets make intuitive sense in the context of a grocery store. Many of these sets are plausible combinations that I can see myself and others buying at a grocery store. Observing the first rule, we can see citrus fruit, other vegetables, and root vegetables being bought together, which is common behavior when purchasing fresh produce. The most significant but simple rule using lift as a metric is beef and root vegetables: two parts of a full meal!


Here is a graphical representation of rules based on our chosen support, confidence, and lift levels filtered to above 2.5. 

```{r, message=FALSE, echo=FALSE, warning=FALSE}
plot(rules, method = "graph")

```




## Image classification with neural networks



