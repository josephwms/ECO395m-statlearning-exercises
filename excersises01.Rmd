---
title: 'ECO 395M: StatLearning Exercise 1'
author: "Joseph Williams, Aahil Navroz, Qi Suqian"
date: "`r Sys.Date()`"
output: html_document
---


## 1) Data visualization: flights at ABIA

For this question we wanted to help flyers (say fliers in 2009) build intuition, or 'rules of thumb', that they can use when choosing between airline companies. To begin we want the data to correspond to recognizable names, so we mapped UniqueCarrier to airline brands or their parent brands.  Here's a breakdown of which major carriers are running the most flights out of ABIA.


```{r setup, message=FALSE, echo=FALSE, warning=FALSE}
library(tidyverse)
library(ggplot2)

#codes = read.csv('./data/airport-codes.csv')
abia_data = read.csv('./data/ABIA.csv', header = TRUE)

#Mapping to tie airlines to carrier codes
carrier_lookup <- data.frame(
  UniqueCarrier = sort(unique(abia_data$UniqueCarrier)),
  Airline = c("Pinnacle", "American", "JetBlue", "North-Western", "Delta", "Alaska", "Frontier", "American", "Celeste", "American", "SkyWest", "United", "SilkAir", "Southwest", "JSX", "United")
)

#Merge code to airline name
abia_data = merge(abia_data, carrier_lookup, by = "UniqueCarrier")

#Lets check using airline == Southwest
data_check = abia_data %>% 
  select(UniqueCarrier, Airline) %>% 
  filter(Airline == "Southwest")
#print(unique(data_check$UniqueCarrier))

#Lets see what the most popular major airlines are
main_airlines = c("United", "Southwest", "JetBlue", "Delta", "American", "Frontier")

flights_by_airline = abia_data %>% 
  filter(Airline %in% main_airlines) %>%
  group_by(Airline) %>%
  summarize(TotalFlights = n())


# Use scale_fill_manual() with the vector of colors
ggplot(flights_by_airline, aes(x = Airline, y = TotalFlights, fill = Airline)) +
  geom_col() +
  labs(title = "Total Flights by Major Airline",
       x = "Airline",
       y = "Flights") +
  theme_minimal()

```


Looks like American and Southwest are king... but can they handle this type of volume.  Lets look at arrival delays for each company. Given these are arrival delays for flights coming into and out of Austin, overall it will be a fine measure for the timeliness of the airline.

```{r, message=FALSE, echo=FALSE, warning=FALSE}

# Converting DepTime to hour of the day
abia_data$DepHour = floor(abia_data$DepTime / 100)

# Calculate average departure delays by airline and hour
avg_delays = abia_data %>%
  filter(Airline %in% main_airlines) %>%
  group_by(Airline, DepHour) %>%
  summarize(AvgArrDelay = mean(ArrDelay, na.rm = TRUE))

# Plotting average departure delay by airline across different hours of the day 
ggplot(avg_delays, aes(x = DepHour, y = AvgArrDelay, color = Airline)) +
  geom_line() +
  geom_point() +
  facet_wrap(~Airline, scales = "fixed", ncol = 3) + 
  theme_minimal() +
  labs(title = "Average Arrival Delays by Airline and Hour",
       x = "Hour of Day",
       y = "Average Delay (Min)") +
  scale_x_continuous(breaks = seq(0, 24, by = 6)) + 
  theme(legend.position = "none")

```

Okay, we're seeing some detail here.  Seems like most companies experience their delays before the hour of 6am.  Since we're looking for rules of thumb.  Lets classify into 'Red Eye' 'Early Morning', '9-5' and 'Evening-Night', and see if we can quantify delay times over periods, rather than specific times.  Some of these averages seem too high, too, lets remove observations where ArrDelay is more than 4 hours, since that usually results in a changed flight for me.  Lets also subtract WeatherDelay, SecurityDelay, and NASDelay from ArrivalDelay since those aren't related to the airline.


```{r, message=FALSE, echo=FALSE, warning=FALSE}

abia_data$DepHour = as.integer(abia_data$DepHour)
#missing_values = sum(is.na(abia_data$DepHour))
#print(missing_values)

abia_data = abia_data[complete.cases(abia_data$DepHour), ]
abia_data = abia_data %>% filter(DepDelay <= 240)

classify_hour <- function(hour) {
  ifelse((hour >= 22 & hour < 24) | (hour >= 0 & hour < 2), "Red-Eye",
         ifelse(hour >= 2 & hour < 9, "Morning",
                ifelse(hour >= 9 & hour < 17, "9-5", "Evening-Night")))
}

abia_data = abia_data %>% 
  mutate(Period = classify_hour(DepHour))


avg_delays_period = abia_data %>%
  filter(Airline %in% main_airlines) %>%
  mutate(WeatherDelay = coalesce(WeatherDelay, 0)) %>%
  mutate(SecurityDelay = coalesce(SecurityDelay, 0)) %>%
  mutate(NASDelay = coalesce(NASDelay, 0)) %>%
  group_by(Airline, Period) %>%
  summarize(AvgArrDelay = mean(ArrDelay - WeatherDelay - SecurityDelay - NASDelay, na.rm = TRUE))



# Define the desired order of the levels for the "Period" variable
desired_order <- c("Morning", "9-5", "Evening-Night", "Red-Eye")

# Reorder the levels of the "Period" variable
avg_delays_period <- avg_delays_period %>%
  mutate(Period = factor(Period, levels = desired_order))

ggplot(avg_delays_period, aes(x = Period, y = AvgArrDelay, fill = Airline)) +
  geom_col() +
  facet_wrap(~Airline, scales = "fixed", ncol = 3) + 
  theme_minimal() +
  labs(title = "Average Arrival Delays by Airline and Period",
       x = "Period",
       y = "Average Delay (Min)") + 
theme(legend.position = "none", axis.text.x = element_text(angle = 45, hjust = 1))

```

Okay now this is more useful!  These average delay times seem too high, though.  Lets check out the data and see if we can remove some outliers for more useful information.

```{r, message=FALSE, echo=FALSE, warning=FALSE}




```

### 2) Wrangling the Olympics

The data in [olympics_top20.csv](../data/olympics_top20.csv) contains information on every Olympic medalist in the top 20 sports by participant count, all the way back to 1896. Use these data to answer the following questions.   (The names of the columns should be self-explanatory.)  

A) What is the 95th percentile of heights for female competitors across all Athletics events (i.e., track and field)?  Note that `sport` is the broad sport (e.g. Athletics) whereas `event` is the specific event (e.g. 100 meter sprint).  
B) Which single women's `event` had the greatest variability in competitor's heights across the entire history of the Olympics, as measured by the standard deviation?  
C) How has the average age of Olympic swimmers changed over time? Does the trend look different for male swimmers relative to female swimmers?  Create a data frame that can allow you to visualize these trends over time, then plot the data with a line graph with separate lines for male and female competitors.  Give the plot an informative caption answering the two questions just posed.



```{r, message=FALSE, echo=FALSE, warning=FALSE}
setwd("/Users/josephwilliams/Desktop/Github_Repos/ECO395m-statlearning-exercises")
olympics = read.csv("./data/olympics_top20.csv")



```



### 3) K-nearest neighbors: cars  

The data in [sclass.csv](../data/sclass.csv) contains data on over 29,000 Mercedes S Class vehicles---essentially every such car in this class that was advertised on the secondary automobile market during 2014.  For websites like Cars.com or Truecar that aim to provide market-based pricing information to consumers, the Mercedes S class is a notoriously difficult case.  There is a huge range of sub-models that are all labeled "S Class,"" from large luxury sedans to high-performance sports cars; one sub-category of S class has even served as the safety car in Formula 1 Races.  Moreover, individual submodels involve cars with many different features.  This extreme diversity---unusual for a single model of car---makes it difficult to provide accurate pricing predictions to consumers.

We'll revisit this data set later in the semester when we've got a larger toolkit for building predictive models.  For now, let's focus on three variables in particular:
- trim: categorical variable for car's trim level, e.g. 350, 63 AMG, etc.  The trim is like a sub-model designation.  
- mileage: mileage on the car
- price: the sales price in dollars of the car

Your goal is to use K-nearest neighbors to build a predictive model for price, given mileage, separately for each of two trim levels: 350 and 65 AMG.  (There are lots of other trim levels that you'll be ignoring for this question.) That is, you'll be treating the 350's and the 65 AMG's as two separate data sets.  (Recall the `filter` command.)  

For each of these two trim levels:
1) Split the data into a training and a testing set.  
2) Run K-nearest-neighbors, for many different values of K, starting at K=2 and going as high as you need to. For each value of K, fit the model to the training set and make predictions on your test set.
3) Calculate the out-of-sample root mean-squared error (RMSE) for each value of K.

For each trim, make a plot of RMSE versus K, so that we can see where it bottoms out.  Then for the optimal value of K, show a plot of the fitted model, i.e. predictions vs. x.  (Again, separately for each of the two trim levels.)

Which trim yields a larger optimal value of K?  Why do you think this is?


```{r, message=FALSE, echo=FALSE, warning=FALSE}


```