---
title: 'ECO 395M: StatLearning Exercise 3'
author: "Joseph Williams, Aahil Navroz, Qi Suqian"
date: "`r Sys.Date()`"
output:
  md_document:
---

```{r, message=FALSE, echo=FALSE, warning=FALSE}

```

## What causes what?

First, listen to [this podcast from Planet Money.](https://www.npr.org/sections/money/2013/04/23/178635250/episode-453-what-causes-what)  Then use your knowledge of statistical learning to answer the following questions.

*1. Why can’t I just get data from a few different cities and run the regression of “Crime” on “Police” to understand how more cops in the streets affect crime? (“Crime” refers to some measure of crime rate and “Police” measures the number of cops in a city.)*

  Selection bias!  Some cities have more police because of more crime and other confounding factors!

*2. How were the researchers from UPenn able to isolate this effect? Briefly describe their approach and discuss their result in the “Table 2” below, from the researchers' paper. *

![Table 2](figures/ex3table2.png)

  The researchers were able to isolate this effect through a natural experiment by observing changing in crime rate between 'terrorist threat' days and normal days.  The intuition here is that terrorism threat status should be independent of day-to-day factors causing crime but WILL increase police presence in the city.  In table2, we see the results on the experiment: that an increased police presence accounts for a decrease of ~6-7 crimes in the city, a result which is statistically significant at the .05 level.

*3. Why did they have to control for Metro ridership? What was that trying to capture?*   

Here, metro ridership is presented as a proxy for foot-traffic in the city.  In the regression above, `Log(midday ridership)` is offered as an omitted variable able to explain variation between crime rate and terrorist threat status.  However, the `High Alert` value and significance is relatively unchanged with the addition of `Log(midday ridership)`, so we can conclude that variation in crime is likely caused by police presence and not by differences in pedestrian behavior.

*4. Below I am showing you "Table 4" from the researchers' paper.  Just focus on the first column of the table. Can you describe the model being estimated here? What is the conclusion?*

![Table 4](figures/ex3table4.png)



## Tree modeling: dengue cases

*Your task is to use _CART_, _random forests_, and _gradient-boosted trees_ to predict dengue cases (or log dengue cases -- your choice, just explain) based on the features available in the data set.  As we usually do, hold out some of the data as a testing set to quantify the performance of these models.  (That is, any cross validation should be done _only_ on the training data, with the testing data held as a final check to compare your best CART model vs. your best random forest model vs. your best boosted tree model.)  Then, for whichever model has the better performance on the testing data, make three partial dependence plots: * 

*- specific_humidity*
*- precipitation_amt*
*- wild card/writer's choice: you choose a feature that looks interesting and make a partial dependence plot for that.*

```{r, message=FALSE, echo=FALSE, warning=FALSE}


```


## Predictive model building: green certification

*Your goal is to build the best predictive model possible for _revenue per square foot per calendar year_, and to use this model to quantify the average change in rental income per square foot (whether in absolute or percentage terms) associated with green certification, holding other features of the building constant. (This might entail, for example, a partial dependence plot, depending on what model you work with here.) Note that revenue per square foot per year is the product of two terms: `rent` and `leasing_rate`!  This reflects the fact that, for example, high-rent buildings with low occupancy may not actually bring in as much revenue as lower-rent buildings with higher occupancy.  *

First, we build all of standard models with limited feature engineering and see which one does best out of the box!  The feature engineering we did perform is excluding non-predictive columns as well as rent and lease rate to remove redundancy.  We also remove any missing values and scale all features.  The models constructed are:
- linear regression
- stepwise
- lasso
- KNN
- descision tree
- random forest
- GBM
- XGBoost

We compare these models by creating an 80% train/test split and forming predictions on the 'test' data set using above models trained using the 'train' data set.  We then calculate RSME for each model:

```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(caret)
library(randomForest)
library(gbm)
library(Metrics)
library(glmnet)
library(xgboost)
library(kknn)
library(rpart)
library(ggplot2)

set.seed(23)
data = read.csv("./data/greenbuildings.csv")

# Calculate revenue per square foot
data$revenue = (data$Rent * data$leasing_rate) 
# Exclude non-predictive columns (e.g., identifiers) as well ass  rent and lease rate to remove redundancy 
features = names(data)[!names(data) %in% c("revenue", "CS_PropertyID", "cluster", "Rent", "leasing_rate")]
data = data[, c(features, "revenue")]

# Remove rows with any missing values (alternative: impute missing values)
data = na.omit(data)

# Scale your features, excluding the target variable
data[features] = scale(data[features])

# Split the data
train_index = createDataPartition(data$revenue, p = 0.8, list = FALSE)
train_data = data[train_index, ]
test_data = data[-train_index, ]

# Model Training


#These are just base line models that require tuning 

# Linear Regression
lm_model = lm(revenue ~ ., data = train_data)

# Additional step-wise model

lm_step = step(lm_model, scope=~(.)^2)

# Lasso Regression
x_train = model.matrix(revenue ~ . - 1, data = train_data)
y_train = train_data$revenue
lasso_model = cv.glmnet(x_train, y_train, alpha = 1)

# KNN
trainControl_knn = trainControl(method = "cv", number = 10)
knn_model = train(revenue ~ ., data = train_data, method = "kknn",
                   trControl = trainControl_knn, tuneLength = 10)

# Decision tree
decision_tree_model = rpart(revenue ~ ., 
                             data = train_data, 
                             method = "anova", 
                             control = rpart.control(cp = 0.001)) # Tuning parameter 'cp' for complexity

# Random Forest
rf_model = randomForest(revenue ~ ., data = train_data, ntree = 1000)

# GBM
gbm_model = gbm(revenue ~ ., data = train_data, distribution = "gaussian",
                 n.trees = 500, interaction.depth = 3, shrinkage = 0.05, cv.folds = 5, verbose = FALSE)

# XGBoost
xgb_data = xgb.DMatrix(data = as.matrix(train_data[, -which(names(train_data) == "revenue")]), label = train_data$revenue)
params = list(booster = "gbtree", eta = 0.1, max_depth = 8, subsample = 0.7, colsample_bytree = 0.7)
xgb_model = xgb.train(params = params, data = xgb_data, nrounds = 10000, early_stopping_rounds = 10, watchlist = list(eval = xgb_data), verbose = 0)

# Prepare test data for prediction
x_test = model.matrix(revenue ~ . - 1, data = test_data)
test_matrix = xgb.DMatrix(data = as.matrix(test_data[, -which(names(test_data) == "revenue")]))

# Generate predictions
lm_predict = predict(lm_model, newdata = test_data)
lm_step_predict = predict(lm_step, newdata = test_data)
lasso_predict = predict(lasso_model, newx = x_test, s = "lambda.min")
knn_predict = predict(knn_model, newdata = test_data)
dt_predict = predict(decision_tree_model, newdata = test_data, type = "vector")
rf_predict = predict(rf_model, newdata = test_data)
gbm_predict = predict(gbm_model, newdata = test_data, n.trees = gbm_model$n.trees)
xgb_predict = predict(xgb_model, newdata = test_matrix)

# Calculate RMSE for each model
actual = test_data$revenue
rmse_values = c(
  lm_rmse = rmse(actual, lm_predict),
  lm_step_rmse = rmse(actual, lm_step_predict),
  lasso_rmse = rmse(actual, lasso_predict),
  knn_rmse = rmse(actual, knn_predict),
  dt_model =  rmse(actual, dt_predict),
  rf_rmse = rmse(actual, rf_predict),
  gbm_rmse = rmse(actual, gbm_predict),
  xgb_rmse = rmse(actual, xgb_predict)
)

print(rmse_values)

# Plotting RMSE values for model comparison
barplot_heights = barplot(rmse_values, 
                           main = "RMSE Comparison Among Models", 
                           ylab = "RMSE Values",
                           las = 2, cex.names = 0.7)



# Assuming 'green_rating' is a binary variable for this example.
# If it's continuous or has more levels, adjust 'green_rating_grid' accordingly.
green_rating_grid <- unique(train_data$green_rating)

# Storage for average predictions
average_preds <- numeric(length(green_rating_grid))

# Loop over each 'green_rating' value
for (i in seq_along(green_rating_grid)) {
  modified_data <- train_data
  modified_data$green_rating <- green_rating_grid[i]
  
  # Ensure the data structure matches training data's
  dmatrix_data <- xgb.DMatrix(data = as.matrix(modified_data[, -which(names(modified_data) == "revenue")]), label = modified_data$revenue)
  
  # Predict and store average prediction
  preds <- predict(xgb_model, dmatrix_data)
  average_preds[i] <- mean(preds)
}

# Create a data frame for plotting
plot_data <- data.frame(green_rating = green_rating_grid, average_pred = average_preds)

# Plot
ggplot(plot_data, aes(x = green_rating, y = average_pred)) +
  geom_line() +
  geom_point() +
  xlab("Green Rating") +
  ylab("Average Predicted Revenue")

average_preds

# This is a graph that depicts the average revenue per foot based on a property's green certification status. Note that since we scaled the features earlier, the green rating goes from -0.3083384 to 3.2427753 instead of 0 to 1. Buildings with green certification (a green rating of 1, which scaled to approximately 3.243) are predicted to generate more revenue per square foot than non-green certified buildings (a green rating of 0, which scaled to approximately -0.308).The difference in predicted revenue between the two categories (green certified and non-green certified) is roughly 2488.989 − 2391.716 = 97.273. This would be the estimated additional revenue per square foot attributed to having a green certification, according to the model's predictions. 

```


## Predictive model building: California housing

Your task is to build the best predictive model you can for `medianHouseValue`, using the other available features.  Write a short report detailing your methods.  Make sure your report includes an estimate for the overall out-of-sample accuracy of your proposed model.  Also include three figures:  

- a plot of the original data, using a color scale to show medianHouseValue (or log medianHouseValue) versus longitude (x) and latitude (y).  
- a plot of your model's predictions of medianHouseValue (or log medianHouseValue) versus longitude (x) and latitude (y).  
- a plot of your model's errors/residuals (or log residuals) versus longitude (x) and latitude (y).



```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(caret)
library(randomForest)
library(gbm)
library(Metrics)
library(gamlr)
library(xgboost)
library(glmnet)
set.seed(23)

CAhousing = read.csv('./data/CAhousing.csv')

## data preprocess
origin = CAhousing
CAhousing$totalRooms = CAhousing$totalRooms/CAhousing$households
CAhousing$totalBedrooms = CAhousing$totalBedrooms/CAhousing$households
CAhousing = data.frame(scale(CAhousing,center = TRUE, scale=TRUE))
CAhousing$medianHouseValue = origin$medianHouseValue

## split the data 
train_index = createDataPartition(CAhousing$medianHouseValue, p = 0.7, list = FALSE)
train_ca = CAhousing[train_index, ]
test_ca = CAhousing[-train_index, ]

## 1 basline model-OLS linear regression
lm_model = lm(medianHouseValue ~., data = train_ca)

## 2 lasso model with 2nd order polynomial

formula = ~ (longitude + latitude + housingMedianAge + totalRooms + 
               totalBedrooms + population + households + medianIncome)^2 + 
                I(longitude)^2 + I(latitude)^2 + I(housingMedianAge)^2 + I(totalRooms)^2 + 
                  I(totalBedrooms)^2 + I(population)^2 +I(households)^2 + I(medianIncome)^2
poly_matrix = model.matrix(formula, data = train_ca[,-9])
poly_matrix = data.frame(scale(poly_matrix,center = TRUE, scale=TRUE))
poly_matrix[,1] = train_ca[,9]
lasso_fit = cv.gamlr(poly_matrix[,-1],poly_matrix[,1],family = "gaussian",penalty="lasso",select="1se")
best_lambda = lasso_fit$lambda.1se
y_train = poly_matrix[,1]
x_train = as.matrix(poly_matrix[,-1])
lasso_model = glmnet(x_train, y_train, alpha = 1, lambda = best_lambda)


## 3 KNN model
trainControl_knn = trainControl(method = "cv", number = 10)
knnFit = train(medianHouseValue ~ ., data = train_ca, method = "knn", 
               trControl = trainControl_knn, tuneGrid = expand.grid(k = 2:40))
k_choice = knnFit$bestTune$k
knn_model = knnreg(x=train_ca[,-9], y=train_ca[,9],k=k_choice)

## 4 Random forest model
rf_model = randomForest(x=train_ca[,-9], y=train_ca[,9],ntree=1000,mtry=3)

## 5 GDBT model
gbm = gbm(medianHouseValue ~ ., data = train_ca, distribution = "gaussian", 
              n.trees = 5000, interaction.depth = 3, shrinkage = 0.05,cv.folds = 10)
gbm_cv = gbm.perf(gbm, method = "cv")# perform bad 
gbm_model = gbm(medianHouseValue ~ ., data = train_ca, distribution = "gaussian", 
                n.trees = gbm_cv, interaction.depth = 3, shrinkage = 0.05)
## 6 XGBoost model
xtrain = xgb.DMatrix(data=as.matrix(train_ca[,-9]),label=train_ca[,9])
params = list(booster = "gbtree", max_depth = 8, subsample = 0.5,eta=0.1)
xgb_model = xgb.train(params = params, data = xtrain,
                       nrounds = 10000, watchlist = list(train = xtrain),
                       early_stopping_rounds = 10)

## compare out-of-sample performance
xtest = xgb.DMatrix(data=as.matrix(test_ca[,-9]),label=test_ca[,9])

poly_matrix_test = model.matrix(formula,data=test_ca[,-9])
poly_matrix_test = data.frame(scale(poly_matrix_test,center = TRUE, scale=TRUE))
poly_matrix_test[,1] = test_ca[,9]
x_test = as.matrix(poly_matrix_test[,-1])

lm_predict = predict(lm_model,newdata=test_ca[,-9])
lasso_predict = predict(lasso_model,newx = x_test, s = best_lambda)
knn_predict = predict(knn_model,newdata=test_ca[,-9])
rf_predict = predict(rf_model,newdata=test_ca[,-9])
gbm_predict = predict(gbm_model,newdata=test_ca[,-9])
xgb_predict = predict(xgb_model,newdata=xtest)

lm_rmse = rmse(test_ca$medianHouseValue,lm_predict)
lasso_rmse = rmse(test_ca$medianHouseValue,lasso_predict)
knn_rmse = rmse(test_ca$medianHouseValue,knn_predict)
rf_rmse = rmse(test_ca$medianHouseValue,rf_predict)
gbm_rmse = rmse(test_ca$medianHouseValue,gbm_predict)
xgb_rmse = rmse(test_ca$medianHouseValue,xgb_predict)

rmse_value = c(lm_rmse,lasso_rmse,knn_rmse,rf_rmse,gbm_rmse,xgb_rmse)
barplot_heights=barplot(
  rmse_value, 
  names.arg = rep("", length(rmse_value)),
  main = "Root Mean Squared Error (RMSE) Comparison",
  ylab = "RMSE Values",
  ylim = c(0, max(rmse_value) * 1.2),
  cex.names = 0.8, cex.lab = 1.1, cex.main = 1.2)
model_names = c("Linear Regression", "Lasso Regression",
                 "KNN Model", "Random Forest Model",
                 "Gradient Boosting Model", "XGBoost Model")
text(x = barplot_heights, y = -max(rmse_value)*0.1, labels = model_names, 
     srt = 25, adj = 1, xpd = TRUE, cex = 0.8)
text(x = barplot_heights, y = rmse_value + max(rmse_value)*0.05, 
     labels = round(rmse_value, 2), srt = 0, pos = 3, cex = 0.8)

```

```{r, message=FALSE, echo=FALSE, warning=FALSE}
#Joe Code.  Lasso tuning

library(gamlr)
library(dplyr)
library(rsample)


Housing = read.csv('./data/CAhousing.csv')

#Include log terms for income, housing value, population and households.  This will standardize data, in practice.  Do we also NEED to standardize data on top of this?

#How are others modifying data?

Housing2 <- Housing %>%
  mutate(
    avgRooms = totalRooms / households,
    avgBedrooms = totalBedrooms / households,
    log_population = log(population),
    log_households = log(households),
    log_medianIncome = log(medianIncome),
    log_medianHouseValue = log(medianHouseValue)
  )

#tract categorical variable... but tracts are already each ind. data point >:/.  Round to single decimal to capture broader location trends

Housing2$latitude_f <- as.factor(round(Housing2$latitude,1))
Housing2$longitude_f <- as.factor(round(Housing2$longitude,1))
Housing2 <- transform(Housing2, tract = interaction(latitude_f, longitude_f, drop = TRUE))

# Perform one-hot encoding
tract_matrix <- model.matrix(~ tract - 1, data = Housing2)

#Combine one-hot encoding to matrix to check results
#Housing2 <- cbind(Housing2, tract_matrix)

#Create df for feature matrix
Housing3 <- Housing2 %>%
   dplyr::select(avgRooms, avgBedrooms, log_population, log_households, log_medianIncome, housingMedianAge)

#Qi code to generate feature matrix
predictor = Housing3[,-1]
variables = names(predictor)
squared_terms = paste0("I(", variables, "^2)", collapse = " + ")
formula = as.formula(paste("~ .^2 +", squared_terms))
predictors =  model.matrix(formula, data = predictor)

#Combine feature matrix and one hot tract variables
lasso_df <- data.frame(predictors, medianHouseValue = Housing2$medianHouseValue)
lasso_features <- cbind(lasso_df, tract_matrix)
lasso_features <- as.data.frame(lasso_features)

#Split into training and testing data
lasso_split = initial_split(lasso_features, prop = 0.8)
lasso_train = training(lasso_split)
lasso_test = testing(lasso_split)

#Prepare training and testing data
x_train = model.matrix(medianHouseValue ~ . - 1, data = lasso_train) # -1 to exclude intercept
y_train = lasso_train$medianHouseValue

x_test = model.matrix(medianHouseValue ~ . - 1, data = lasso_test)
y_test = lasso_test$medianHouseValue

lasso_1 = cv.gamlr(x_train, y_train, nfold=10, verb=TRUE)

lasso1_rsme = sqrt(lasso_1$cvm[lasso_1$seg.1se]) %>% round(4)

#Data exploration.  Important later!
significant_vars = rownames(coef(lasso_1, s = '1se'))[coef(lasso_1, s = '1se')[,1]!= 0]

# #Testing vs other models
# #Just feature matrix w lat/long
# features_2 = cbind(predictors, Housing2$longitude, Housing2$latitude)
# lasso_2 = cv.gamlr(features_2, Housing2$medianHouseValue, nfold=10, verb=TRUE)
# lasso2_rsme = sqrt(lasso_2$cvm[lasso_2$seg.1se]) %>% round(4)
# 
# #Just feature matrix
# features_3 = predictors
# lasso_3 = cv.gamlr(features_3, Housing2$medianHouseValue, nfold=10, verb=TRUE)
# lasso3_rsme = sqrt(lasso_3$cvm[lasso_3$seg.1se]) %>% round(4)
# 
# #Just simple variables w lat/long
# features_4 = Housing2 %>% dplyr::select(avgRooms, avgBedrooms, log_population, log_households, log_medianIncome, housingMedianAge, latitude, longitude)
# lasso_4 = cv.gamlr(features_4, Housing2$medianHouseValue, nfold=10, verb=TRUE)
# lasso4_rsme = sqrt(lasso_4$cvm[lasso_4$seg.1se]) %>% round(4)
# 
# #Just simple variables
# features_5 = Housing2 %>% dplyr::select(avgRooms, avgBedrooms, log_population, log_households, log_medianIncome, housingMedianAge)
# lasso_5 = cv.gamlr(features_5, Housing2$medianHouseValue, nfold=10, verb=TRUE)
# lasso5_rsme = sqrt(lasso_5$cvm[lasso_5$seg.1se]) %>% round(4)

```




