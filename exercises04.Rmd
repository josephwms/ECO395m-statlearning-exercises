---
title: 'ECO 395M: StateLearning Exercise 4'
author: "Aahil Navroz, Joseph Williams, Qi Suqian"
date: "`r Sys.Date()`"
output: 
  md_document:
---
# ECO 395M: StatLearning Exercise 3

Aahil Navroz, Joseph Williams, Qi Suqian

04/22/2023

## Clustering and PCA

```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(ggcorrplot)
library(tidyverse)
library(ClusterR)
library(arules)
library(arulesViz)

wine = read.csv("./data/wine.csv")
chemical = wine[,-(12:13)]
chemical = scale(chemical, center=TRUE, scale=TRUE)
color_sample = wine[,-12]
color_sample$color = as.numeric(color_sample$color == "red")
quality_sample = wine[,-13]
ggcorrplot(cor(color_sample),lab = TRUE,hc.order = TRUE)
ggcorrplot(cor(quality_sample),lab = TRUE,hc.order = TRUE)
# it seems total.sulfur.dioxide, volatile.acidity and chlorides are the 3 most related variables to color
# alcohol is the most related variable to quality
# summary statistic for red and white
```

## Market segmentation

*Use the data to come up with some interesting, well-supported insights about the audience and give your client some insight as to how they might position their brand to maximally appeal to each market segment.*

To get a basic idea of our data lets start with a two-way correlation plot.

```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(dplyr)

market = read.csv("./data/social_marketing.csv")
features = market %>% select(-X)
ggcorrplot(cor(features),
           lab = TRUE,
           hc.order = TRUE,
           lab_size = .1,
           tl.cex=8
           )      # Vertical alignment

```

Great! So already we're seeing some clusters.  I notice see two way correlation within the following groups: 

* group1_familyvalues: `parenting`, `religion`, `sports_fandom`, `food`, `school`, `family`
* group2_collegeboy: `college_uni`, `online_gaming`, `sports_playing`
* group3_fashionable: `beauty`, `cooking`, `fashion`
* group4_yuppie: `personal_fitness`, `health_nutrition`, `outdoors`
* group5_neoliberal: `politics`, `travel`, `computers`
* group6_socialyte: `shopping`, `chatter`, `photo_sharing`

Lets organize counts to see how many followers had at least two or more tweets in at least 2, (or 3 for family_values and socialyte) variables of each cluster.  Lets also filter out users who are in more than 3 of these respective groups to eliminate generalists and get a better personality portrait of our followers.

```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(ggplot2)

market1 <- market %>% 
  rowwise() %>%
  mutate(
    group1_familyvalues = as.numeric(sum(parenting > 1, religion > 1, sports_fandom > 1, food > 1, school > 1, family > 1) >= 3),
    group2_collegeboy = as.numeric(sum(college_uni > 1, online_gaming > 1, sports_playing > 1) >= 2),
    group3_fashionable = as.numeric(sum(beauty > 1, cooking > 1, fashion > 1) >= 2),
    group4_yuppie = as.numeric(sum(personal_fitness > 1, health_nutrition > 1, outdoors > 1) >= 2),
    group5_neoliberal = as.numeric(sum(politics > 1, travel > 1, computers > 1) >= 2),
    group6_socialyte = as.numeric(sum(shopping > 1, chatter > 1, photo_sharing > 1) >= 3),
    num_groups = sum(group1_familyvalues,group2_collegeboy,group3_fashionable,group4_yuppie,group5_neoliberal,group6_socialyte)
  ) %>%
  ungroup()

group_counts1 <- market1 %>%
  filter(num_groups<4) %>%
  summarise(
    group1_familyvalues = sum(group1_familyvalues, na.rm = TRUE),
    group2_collegeboy = sum(group2_collegeboy, na.rm = TRUE),
    group3_fashionable = sum(group3_fashionable, na.rm = TRUE),
    group4_yuppie = sum(group4_yuppie, na.rm = TRUE),
    group5_neoliberal = sum(group5_neoliberal, na.rm = TRUE),
    group6_socialyte = sum(group6_socialyte, na.rm = TRUE)
  ) %>%
  pivot_longer(
    cols = everything(),
    names_to = "group",
    values_to = "user_count"
  )

# group_counts2 <- market1 %>%
#   filter(num_groups<3) %>%
#   summarise(
#     group1_familyvalues = sum(group1_familyvalues, na.rm = TRUE),
#     group2_collegeboy = sum(group2_collegeboy, na.rm = TRUE),
#     group3_fashionable = sum(group3_fashionable, na.rm = TRUE),
#     group4_yuppie = sum(group4_yuppie, na.rm = TRUE),
#     group5_neoliberal = sum(group5_neoliberal, na.rm = TRUE),
#     group6_socialyte = sum(group6_socialyte, na.rm = TRUE)
#   ) %>%
#   pivot_longer(
#     cols = everything(),
#     names_to = "group",
#     values_to = "user_count"
#   )

# Create the bar plot
ggplot(group_counts1, aes(x = group, y = user_count)) +
  geom_bar(stat = "identity") +
  ggtitle("Counts by Group for Users in Max 3 Groups") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# # Create the bar plot
# ggplot(group_counts2, aes(x = group, y = user_count)) +
#   geom_bar(stat = "identity") +
#   ggtitle("Counts by Group for Users in Max 2 Groups") +
#   ylim(0, 2000) +
#   theme_minimal() +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Over 1500 users are in the loosely constructed `familyvalues', 'yuppie', and 'socialyte' groups, respectively.  Even with filtering efforts, however, many users are likely counted 2 or even 3 times. Before we go farther in this direction, lets shift to machine learning algorithms so that our clusters are definite and exhaustive.  We'll start with K-means and K_means++ clustering with K=5-7 clusters, since we see 6 key groups off the bat.  See below visualizations and counts per cluster.


```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(ClusterR)  # for kmeans++
library(ggrepel)

#for K++
# Extract and round centroids
# centroids <- clust2$centroids
# colnames(centroids) <- colnames(ft)
# clust2_rounded <- round(centroids, 2)
# print(clust2_rounded)

# Data preprocessing
ft = market[,-1]  # Exclude the first column
ft = scale(ft, center=TRUE, scale=TRUE)

# Execute kmeans
set.seed(1)
k_values <- c(5, 7, 6)

for (k in k_values) {
  clust = kmeans(ft, k, nstart=50)
  
  # Extract and round centroids
  centroids <- round(clust$centers, 2)
  
  #return cluster data for k=6
  # if (k==6){
  #   print(centroids)
  # }
    
  
  # Convert centroids to a dataframe
  centroids_df <- as.data.frame(centroids)
  centroids_df <- centroids_df %>%
    rownames_to_column(var = "centroid")
  
  # Convert centroids dataframe to long format for plotting
  centroids_long <- centroids_df %>%
    gather(key = "variable", value = "value", -centroid)
  
  # Plot pie charts for each centroid
  pie_plots <- ggplot(centroids_long, aes(x = "", y = value, fill = variable)) +
    geom_bar(stat = "identity", width = 1) +
    coord_polar("y", start=0) +
    facet_wrap(~centroid, ncol = 3) +
    labs(title = paste("Pie Charts of Centroids, K =", k),
         fill = "Variable") +
    theme_void()
  
  # Add labels for variables with centroid value > 1 without overlap
  label_data <- centroids_long %>%
    filter(value > 1)
  
  pie_plots <- pie_plots +
    geom_text_repel(data = label_data, aes(label = variable, y = value), 
                    position = position_stack(vjust = 0.5), 
                    size = 3) +
    theme(legend.position = "none")
  
  # Count number of users in each cluster
  cluster_counts <- as.data.frame(table(clust$cluster))
  colnames(cluster_counts) <- c("Cluster", "Users")
  
  
  print(pie_plots)
  print(cluster_counts)
  
}




```

We are able to confirm using unsupervised learning pretty much the same clusters we identified using intuition and basic tools. Labeled variables have centroid values over a certain, relatively-high threshold.  Our results use standard K-means but we've verified that results are repeatable with K-means++ start up. Regarding selection of K we notice the following points:

- Each K identifies a 'spam' category where no variables are dominant.
- K=5 splits up our 'fashionable' and 'socialyte' clusters between the four non-spam groups, so that the only major feature of one of the clusters is `photo_sharing`.  
- K=6,7 recovers the 'fashionable' cluster, and some of the 'socialyte' cluster. Its likely that the 'socialyte' characteristics `shopping`, `chatter` and `photo sharing` represent popular uses of Twitter which are more easily distributed between users in other categories.

Altogether, its clear that K=6 weeds out the most spam posts and maintains an even distribution between the other categories, a key requirement of clustering. Most importantly, it aligns incredibly well with our opening analysis. Next, our key insights will rely on K-means clustering using K=6 and identify five distinct non-spam groups.


#### Insights & Recommended Steps
We've identified 5 roughly even-sized market segments.  Here are the two largest in descending order:

- **Health-conscious adults (likely mid-twenties to thirties)**. In my city we call these yoga enthusiasts and REI shoppers 'yuppies', short for 'young professionals'.  This market segment shared twitter engagement in `health_nutrition`, `personal_fitness`, `outdoors`.  This segment also showed notable interest for `eco`, though it did not meet the display threshold.  We recommend an approach that shows sustainability efforts, and connects your product to outdoor engagement and mental health.

- **Traditional Americans**. Don't forget about the heartland, the silent majority, your minivan moms and sports bar dads. This market segment showed over-threshold engagement with more categories than any other group, by far, indicating they are 'classic American' consumers- not part of any niche group.  Key characteristics align with traditional values, and include `family`, `food`, `religion`, `sports fandom`, `school` and `parenting`. To appeal to this group, show that your product could easily find its way to a children's soccer game or family reunion.

Overall, to appeal to both groups and maximize market outreach, perhaps you are the beverage of choice for the modern parent... but not *too* modern. Perhaps its being consumed on a good ol' fashioned camping trip. Don't forget to put ice in the cooler!






## Association rules for grocery purchases

*Find some interesting association rules for these shopping baskets. Pick your own thresholds for lift and confidence; just be clear what these thresholds are and how you picked them. Do your discovered item sets make sense? Present your discoveries using an interesting visualization or two, along with no more than one page of typed text.*


We'll begin with some initial data wrangling in order to view the data.  See below head() for first 5 baskets.

```{r, message=FALSE, echo=FALSE, warning=FALSE}



# First we need to transform the text file into a readable format to determine association rules for shopping baskets. 

# Read the data from the file
groceries_raw = readLines("./data/groceries.txt")

# Split each line into items, remove duplicates, and create a list
groceries_list = lapply(groceries_raw, function(line) unique(strsplit(line, ",")[[1]]))

# Convert the list to transaction class
groceries = as(groceries_list, "transactions")

# Preview the data
inspect(head(groceries, n = 5))

```


Next, we'll initialize association analysis with very low support and confidence levels and further tuned from there depending on the distributions we saw while plotting the rules. Based on the spread of confidence and lift, we changed values in order to find that sweet spot of confidence which is not too restrictive, and has a lift value which will enable us to focus on more significant rules. Eventually, I arrived at the value of 0.01 for support and set confidence to 0.25.


```{r, message=FALSE, echo=FALSE, warning=FALSE, results='hide'}
rules = apriori(groceries, parameter = list(supp = 0.01, conf = 0.25, minlen = 2))

```


```{r, message=FALSE, echo=FALSE, warning=FALSE}

plot(rules, jitter = 0)

```

170 is quite a large number for rules in this case. I would like more to analyze more significant associations so I will only focus on lifts that are greater than 2.5.

```{r, message=FALSE, echo=FALSE, warning=FALSE}
rules = subset(rules, lift > 2.5)

inspect(sort(rules, by = "lift"))

```

Looking at the rules sorted by lift, many of these item sets make intuitive sense in the context of a grocery store. Many of these sets are plausible combinations that I can see myself and others buying at a grocery store. Observing the first rule, we can see citrus fruit, other vegetables, and root vegetables being bought together, which is common behavior when purchasing fresh produce. The most significant but simple rule using lift as a metric is beef and root vegetables: two parts of a full meal!


Here is a graphical representation of rules based on our chosen support, confidence, and lift levels filtered to above 2.5. 

```{r, message=FALSE, echo=FALSE, warning=FALSE}
plot(rules, method = "graph")

```




## Image classification with neural networks



