---
title: 'ECO 395M: StatLearning Exercise 3'
author: "Joseph Williams, Aahil Navroz, Qi Suqian"
date: "`r Sys.Date()`"
output:
  html_document:
---

```{r, message=FALSE, echo=FALSE, warning=FALSE}

```

## What causes what?
*1. Why can’t I just get data from a few different cities and run the regression of “Crime” on “Police” to understand how more cops in the streets affect crime? (“Crime” refers to some measure of crime rate and “Police” measures the number of cops in a city.)*

Selection bias!  Some cities have more police because of more crime and other confounding factors!

*2. How were the researchers from UPenn able to isolate this effect? Briefly describe their approach and discuss their result in the “Table 2” below, from the researchers' paper. *

![Table 2](figures/ex3table2.png){width=50%}

  The researchers were able to isolate this effect through a natural experiment by observing changing in crime rate between 'terrorist threat' days and normal days.  The intuition here is that terrorism threat status should be independent of day-to-day factors causing crime but WILL increase police presence in the city.  In table2, we see the results on the experiment: that an increased police presence accounts for a decrease of ~6-7 crimes in the city, a result which is statistically significant at the .05 level.

*3. Why did they have to control for Metro ridership? What was that trying to capture?*   

Here, metro ridership is presented as a proxy for foot-traffic in the city.  In the regression above, `Log(midday ridership)` is offered as an omitted variable able to explain variation between crime rate and terrorist threat status.  However, the `High Alert` value and significance is relatively unchanged with the addition of `Log(midday ridership)`, so we can conclude that variation in crime is likely caused by police presence and not by differences in pedestrian behavior.

*4. Below I am showing you "Table 4" from the researchers' paper.  Just focus on the first column of the table. Can you describe the model being estimated here? What is the conclusion?*

![Table 4](figures/ex3table4.png){width=50%}



## Tree modeling: dengue cases

*Your task is to use _CART_, _random forests_, and _gradient-boosted trees_ to predict dengue cases (or log dengue cases -- your choice, just explain) based on the features available in the data set.  As we usually do, hold out some of the data as a testing set to quantify the performance of these models.  (That is, any cross validation should be done _only_ on the training data, with the testing data held as a final check to compare your best CART model vs. your best random forest model vs. your best boosted tree model.)  Then, for whichever model has the better performance on the testing data, make three partial dependence plots: * 

*- specific_humidity*
*- precipitation_amt*
*- wild card/writer's choice: you choose a feature that looks interesting and make a partial dependence plot for that.*

We first impute the data with KNN method and scale all varaibles except the dependent one.
```{r, message=FALSE, echo=FALSE, warning=FALSE}
dengue = read.csv("./data/dengue.csv")
library(rpart)
library(caret)
library(RANN)
library(boot)
library(randomForest)
library(gbm)
library(Metrics)
library(pdp)
set.seed(23)

# Impute the missing value of the data with KNN
preProcValues = preProcess(dengue, method = 'knnImpute')
dataImputed = predict(preProcValues, dengue)

# change the data type to numeric or factor and change the dependent variable to be positive
dataImputed$city = as.factor(dataImputed$city)
dataImputed$season = as.factor(dataImputed$season)
dataImputed$total_cases = dengue$total_cases

# split the data to training data and test data
train_index = createDataPartition(dataImputed$total_cases, p = 0.7, list = FALSE)
train_data = dataImputed[train_index, ]
test_data = dataImputed[-train_index, ]
```

Now we train the CART model with the training data and select the best parameters. Since the sample size is not big, we choose the default minsplit and use cv to choose the best cp
```{r, message=FALSE, echo=FALSE, warning=FALSE}
###### 1 CART model
fitControl1 = trainControl(method = "repeatedcv", number = 10, repeats = 10)
tunedf = data.frame(.cp = seq(0.001, 0.1, length = 20))
treemodel = train(x = train_data[, -3], y = train_data[, 3], method = 'rpart', 
                  trControl = fitControl1, tuneGrid = tunedf, metric = "RMSE")
plot(treemodel)
best_cp = tunedf$.cp[3]
cart_model = rpart(total_cases ~., data=train_data, control=rpart.control(cp=best_cp))
```
From the plot, we choose cp to be 0.011 according to minimium criterion. 
Then we move to the random forest model.We choose the number of bootstrapped sample to be 2000 to avoid selection. However, we choose the number of features to sample within each bootstrapped sample by OOB method.
```{r, message=FALSE, echo=FALSE, warning=FALSE}
###### 2 random forest model
fitControl2 = trainControl(method = "oob", number = 10)
mtryGrid = expand.grid(.mtry = seq(from = 1, to = ncol(train_data)-1, by = 1))
rfmodel = train(x = train_data[, -3], y = train_data[, 3], method = "rf",
                 trControl = fitControl2, tuneGrid = mtryGrid, metric = "RMSE")
plot(rfmodel)
best_mtry = mtryGrid$.mtry[5]
rf_model = randomForest(x=train_data[,-3], y=train_data[,3],ntree=2000,mtry=best_mtry)
```
From the plot, we choose mtry to be 5 according to the minimum criterion.
Then we consider the Gradient Boosting Decision Tree model(GBDT). We choose large number of trees to avoid selection for n.trees. For the distribution of y, we also try for poisson distribution besides the common choice, gaussian distribution, since the outcome total_cases is a sum of count and we assume it may follows a poisson distribution. Then we choose the interaction.depth and shrinkage by CV. We choose the default value for n.minobsinnode as 10 becasuse of the small sample sie and we don't want each tree to go too deep which may lead to overfittness.(The selection may take a long time and u can just run the last two command)
```{r, message=FALSE, echo=FALSE, warning=FALSE}
###### 3 GBDT model
fitControl3 = trainControl(method = "cv",  
                              number = 10)    
gbm_grid = expand.grid(interaction.depth = 1:8,  
                         n.trees = 2000,          
                         shrinkage = c(0.001,0.005,0.01, 0.05, 0.1),
                       n.minobsinnode = 10)
norm_model = train(total_cases ~ ., data = train_data,
                   method = "gbm",
                   trControl = fitControl3,
                   tuneGrid = gbm_grid,
                   verbose = FALSE,
                   metric = "RMSE",  
                   distribution = "gaussian")
poi_model = train(total_cases ~ ., data = train_data,
                  method = "gbm",
                  trControl = fitControl3,
                  tuneGrid = gbm_grid,
                  verbose = FALSE,
                  metric = "RMSE",  
                  distribution = "poisson")
gbm_nor_model = gbm(total_cases ~ ., data = train_data, distribution = "gaussian", 
                n.trees=2000,interaction.depth = 7,shrinkage = 0.01,n.minobsinnode = 10)
gbm_poi_model = gbm(total_cases ~ ., data = train_data, distribution = "poisson", 
                    n.trees = 2000, interaction.depth = 8, shrinkage = 0.005,n.minobsinnode = 10)
```
From our tunning result, for gaussian model, depth is 7 and shrinkage rate is 0.01; for poisson model, depth is 8 and shrinkage rate is 0.005.
Then we use the test data to measure the performance for all these four models by RMSE.
```{r, message=FALSE, echo=FALSE, warning=FALSE}
####### 4 compare the out-of-sample performance
cart_predict = predict(cart_model, newdata=test_data)
rf_predict = predict(rf_model, newdata=test_data)
gbm_nor_predict = predict(gbm_nor_model, newdata=test_data)
gbm_poi_predict = predict(gbm_poi_model, newdata=test_data)
# calculate the rmse
cart_rmse = rmse(test_data$total_cases,cart_predict)
rf_rmse = rmse(test_data$total_cases,rf_predict)
gbm_nor_rmse = rmse(test_data$total_cases,gbm_nor_predict)
gbm_poi_rmse = rmse(test_data$total_cases,gbm_poi_predict)
# make a bar plot for all 4 models
rmse_value = c(cart_rmse,rf_rmse,gbm_nor_rmse,gbm_poi_rmse)
barplot_heights=barplot(
  rmse_value, 
  names.arg = rep("", length(rmse_value)),
  main = "Root Mean Squared Error (RMSE) Comparison",
  ylab = "RMSE Values",
  ylim = c(0, max(rmse_value) * 1.2),
  cex.names = 0.8, cex.lab = 1.1, cex.main = 1.2)
model_names = c("CART Model", "Random Forest Model",
                "GBDT Normal", "GBDT Poisson")
text(x = barplot_heights, y = -max(rmse_value)*0.1, labels = model_names, 
     srt = 25, adj = 1, xpd = TRUE, cex = 0.8)
text(x = barplot_heights, y = rmse_value + max(rmse_value)*0.05, 
     labels = round(rmse_value, 2), srt = 0, pos = 3, cex = 0.8)
```
The plot shows our CART model has the lowest RMSE. Then we make 3 partial dependence plots for CART model, specific_humidity, precipitation_amt and tdtr_k.
```{r, message=FALSE, echo=FALSE, warning=FALSE}
##### 5 PD plot
pd1 = partial(cart_model, pred.var = "specific_humidity")
pd1_df = as.data.frame(pd1)
ggplot(pd1_df, aes(x = specific_humidity, y = yhat)) +
  geom_line() +
  labs(title = 'Partial Dependence on "Specific_humidity"', x = "Specific_humidity")

pd2 = partial(cart_model, pred.var = "precipitation_amt")
pd2_df = as.data.frame(pd2)
ggplot(pd2_df, aes(x = precipitation_amt, y = yhat)) +
  geom_line() +
  labs(title = 'Partial Dependence on "Precipitation_amt"', x = "Precipitation_amt")

pd3 = partial(cart_model, pred.var = "tdtr_k")
pd3_df = as.data.frame(pd3)
ggplot(pd3_df, aes(x = tdtr_k, y = yhat)) +
  geom_line() +
  labs(title = 'Partial Dependence on "tdtr_k"', x = "tdtr_k")

```
## Predictive model building: green certification

*Your goal is to build the best predictive model possible for _revenue per square foot per calendar year_, and to use this model to quantify the average change in rental income per square foot (whether in absolute or percentage terms) associated with green certification, holding other features of the building constant. (This might entail, for example, a partial dependence plot, depending on what model you work with here.) Note that revenue per square foot per year is the product of two terms: `rent` and `leasing_rate`!  This reflects the fact that, for example, high-rent buildings with low occupancy may not actually bring in as much revenue as lower-rent buildings with higher occupancy.  *

First, we build all of standard models with limited feature engineering and see which one does best out of the box!  The feature engineering we did perform is excluding non-predictive columns as well as rent and lease rate to remove redundancy.  We also remove any missing values and scale all features.  The models constructed are:
- linear regression
- stepwise
- lasso
- KNN
- descision tree
- random forest
- GBM
- XGBoost

We compare these models by creating an 80% train/test split and forming predictions on the 'test' data set using above models trained using the 'train' data set.  We then calculate RSME for each model:

```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(caret)
library(randomForest)
library(gbm)
library(Metrics)
library(glmnet)
library(xgboost)
library(kknn)
library(rpart)
library(ggplot2)
library(pdp)

set.seed(23)
data = read.csv("./data/greenbuildings.csv")

# Calculate revenue per square foot
data$revenue = (data$Rent * data$leasing_rate) 
# Exclude non-predictive columns (e.g., identifiers) as well ass  rent and lease rate to remove redundancy 
features = names(data)[!names(data) %in% c("revenue", "CS_PropertyID", "cluster", "Rent", "leasing_rate")]
data = data[, c(features, "revenue")]

# Remove rows with any missing values (alternative: impute missing values)
data = na.omit(data)

# Scale your features, excluding the target variable and non_binary features
data[features] = scale(data[features])

# Split the data
train_index = createDataPartition(data$revenue, p = 0.8, list = FALSE)
train_data = data[train_index, ]
test_data = data[-train_index, ]

# # Model Training
# #These are just base line models that require tuning 
# 
# # Linear Regression
# lm_model = lm(revenue ~ ., data = train_data)
# 
# # Additional step-wise model
# lm_step = step(lm_model, scope=~(.)^2)
# 
# #Lasso Regression
# x_train = model.matrix(revenue ~ . - 1, data = train_data)
# y_train = train_data$revenue
# lasso_model = cv.glmnet(x_train, y_train, alpha = 1)
# 
# # KNN
# trainControl_knn = trainControl(method = "cv", number = 10)
# knn_model = train(revenue ~ ., data = train_data, method = "kknn",
#                    trControl = trainControl_knn, tuneLength = 10)

# # Decision tree
# decision_tree_model = rpart(revenue ~ .,
#                              data = train_data,
#                              method = "anova",
#                              control = rpart.control(cp = 0.001)) # Tuning parameter 'cp' for complexity
# 
# # GBM
# gbm_model = gbm(revenue ~ ., data = train_data, distribution = "gaussian",
#                  n.trees = 500, interaction.depth = 3, shrinkage = 0.05, cv.folds = 5, verbose = FALSE)

#See code below for missing process flow

##Code to generate below graphic
# Create a data frame
# rmse_values <- data.frame(
#   Model = c("lm", "lm_step", "lasso", "knn", "dt", "rf", "gbm", "xgb"),
#   RMSE = c(lm_rmse, lm_step_rmse, lasso_rmse, knn_rmse, dt_rmse, rf_rmse, gbm_rmse, xgb_rmse)
# )

# Plotting RMSE values for model comparison
# barplot_heights = barplot(rmse_df, 
#                            main = "RMSE Comparison Among Models", 
#                            ylab = "RMSE Values",
#                            las = 2, cex.names = 0.7)


```

![Table 4](figures/Initial_model_comparison.png)

We see that random forest and xgboost do the best out of box.  With some tuning, we are able to identify xgboost as our best possible model.


```{r, message=FALSE, echo=FALSE, warning=FALSE}
# Random Forest
rf_model = randomForest(revenue ~ ., data = train_data, ntree = 1000)

# XGBoost
xgb_data = xgb.DMatrix(data = as.matrix(train_data[, -which(names(train_data) == "revenue")]), label = train_data$revenue)
params = list(booster = "gbtree", eta = 0.1, max_depth = 8, subsample = 0.7, colsample_bytree = 0.7)
xgb_model = xgb.train(params = params, data = xgb_data, nrounds = 10000, early_stopping_rounds = 10, watchlist = list(eval = xgb_data), verbose = 0)

# Prepare test data for prediction
x_test = model.matrix(revenue ~ . - 1, data = test_data)
test_matrix = xgb.DMatrix(data = as.matrix(test_data[, -which(names(test_data) == "revenue")]))

#actual revenue values per house
actual = test_data$revenue

# Generate predictions
#lm_predict = predict(lm_model, newdata = test_data)
#lm_step_predict = predict(lm_step, newdata = test_data)
#lasso_predict = predict(lasso_model, newx = x_test, s = "lambda.min")
#knn_predict = predict(knn_model, newdata = test_data)
#dt_predict = predict(decision_tree_model, newdata = test_data, type = "vector")
rf_predict = predict(rf_model, newdata = test_data)
#gbm_predict = predict(gbm_model, newdata = test_data, n.trees = gbm_model$n.trees)
xgb_predict = predict(xgb_model, newdata = test_matrix)

#Calcuate RMSE for each model

#lm_rmse <- rmse(actual, lm_predict)
#lm_step_rmse <- rmse(actual, lm_step_predict)
#lasso_rmse <- rmse(actual, lasso_predict)
#knn_rmse <- rmse(actual, knn_predict)
#dt_rmse <- rmse(actual, dt_predict)
rf_rmse <- rmse(actual, rf_predict)
#gbm_rmse <- rmse(actual, gbm_predict)
xgb_rmse <- rmse(actual, xgb_predict)

# Create a data frame
rmse_values <- data.frame(
   Model = c("rf", "xgb"),
   RMSE = c(rf_rmse, xgb_rmse)
 )

ggplot(rmse_values, aes(x = Model, y = RMSE)) +
  geom_col(width = 0.6) +
  coord_cartesian(ylim = c(500, max(rmse_values$RMSE) * 1.1)) +
  ggtitle("RMSE Comparison Among Models") +
  theme_minimal()

# Print the minimum lambda value used
cat("xgboost rsme:", xgb_rmse)

```

In order to answer our principal question of whether or not a 'green rating' has a significant effect on building revenue we calculate a partial dependence plot for green_rating vs revenue.


```{r, message=FALSE, echo=FALSE, warning=FALSE}

# Generate the partial dependence plot data
pdp_green = partial(
  xgb_model, 
  pred.var = "green_rating", 
  train = train_data[, -which(names(train_data) == "revenue")], 
  grid.resolution = 100,
  plot = FALSE  # Do not create the plot automatically
)

# Plot
ggplot(pdp_green, aes(x = green_rating, y = yhat)) +
  geom_line() +
  geom_point() +
  xlab("Green Rating") +
  ylab("Average Predicted Revenue") +
  ggtitle("Partial Dependence of Revenue on Green Rating")


# This is a graph that depicts the average revenue per foot based on a property's green certification status. Note that since we scaled the features earlier, the green rating goes from -0.3083384 to 3.2427753 instead of 0 to 1. Buildings with green certification (a green rating of 1, which scaled to approximately 3.243) are predicted to generate more revenue per square foot than non-green certified buildings (a green rating of 0, which scaled to approximately -0.308).The difference in predicted revenue between the two categories (green certified and non-green certified) is roughly 2488.989 − 2391.716 = 97.273. This would be the estimated additional revenue per square foot attributed to having a green certification, according to the model's predictions. 
```


## Predictive model building: California housing

Your task is to build the best predictive model you can for `medianHouseValue`, using the other available features.  Write a short report detailing your methods.  Make sure your report includes an estimate for the overall out-of-sample accuracy of your proposed model.  Also include three figures:  

- a plot of the original data, using a color scale to show medianHouseValue (or log medianHouseValue) versus longitude (x) and latitude (y).  
- a plot of your model's predictions of medianHouseValue (or log medianHouseValue) versus longitude (x) and latitude (y).  
- a plot of your model's errors/residuals (or log residuals) versus longitude (x) and latitude (y).


We first scale the data except the dependent variable and split the sample into train set ans test set.
```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(caret)
library(randomForest)
library(gbm)
library(Metrics)
library(gamlr)
library(xgboost)
library(glmnet)
set.seed(23)

CAhousing = read.csv('./data/CAhousing.csv')

## data preprocess
origin = CAhousing
CAhousing$totalRooms = CAhousing$totalRooms/CAhousing$households
CAhousing$totalBedrooms = CAhousing$totalBedrooms/CAhousing$households
CAhousing = data.frame(scale(CAhousing,center = TRUE, scale=TRUE))
CAhousing$medianHouseValue = origin$medianHouseValue

## split the data 
train_index = createDataPartition(CAhousing$medianHouseValue, p = 0.7, list = FALSE)
train_ca = CAhousing[train_index, ]
test_ca = CAhousing[-train_index, ]
```

We tried 6 models to predict the value for the median house value: a baseline linear model, lasso model with 2nd order interaction terms, KNN model, Random Forest model, GBDT model and XGBoost model.We first run the linear models.
```{r, message=FALSE, echo=FALSE, warning=FALSE}
## 1 baseline model-OLS linear regression
lm_model = lm(medianHouseValue ~., data = train_ca)

## 2 lasso model with 2nd order polynomial
formula = ~ (longitude + latitude + housingMedianAge + totalRooms + 
               totalBedrooms + population + households + medianIncome)^2 + 
                I(longitude)^2 + I(latitude)^2 + I(housingMedianAge)^2 + I(totalRooms)^2 + 
                  I(totalBedrooms)^2 + I(population)^2 +I(households)^2 + I(medianIncome)^2
poly_matrix = model.matrix(formula, data = train_ca[,-9])
poly_matrix = data.frame(scale(poly_matrix,center = TRUE, scale=TRUE))
poly_matrix[,1] = train_ca[,9]
lasso_fit = cv.gamlr(poly_matrix[,-1],poly_matrix[,1],family = "gaussian",penalty="lasso",select="1se")
best_lambda = lasso_fit$lambda.1se
y_train = poly_matrix[,1]
x_train = as.matrix(poly_matrix[,-1])
lasso_model = glmnet(x_train, y_train, alpha = 1, lambda = best_lambda)
```

Then we look at the KNN model and random forest model. We use CV to choose the best k for KNN model. For random forest model, we use 1000 trees and choose the mtry as the default one.
```{r, message=FALSE, echo=FALSE, warning=FALSE}
## 3 KNN model
trainControl_knn = trainControl(method = "cv", number = 10)
knnFit = train(medianHouseValue ~ ., data = train_ca, method = "knn", 
               trControl = trainControl_knn, tuneGrid = expand.grid(k = 2:40))
k_choice = knnFit$bestTune$k
knn_model = knnreg(x=train_ca[,-9], y=train_ca[,9],k=k_choice)

## 4 Random forest model
rf_model = randomForest(x=train_ca[,-9], y=train_ca[,9],ntree=1000)
```

We explore 2 Boosting tree model here, GBDT and XGboost model. We first look at the GBDT model. We use CV to select the best interaction depth and shrinkage rate. We set the n.trees as 1000 since we think it's sufficient large and we set the distribution as gaussian. Since the sample size is small, we set the n.minobsinnode to be 10.
```{r, message=FALSE, echo=FALSE, warning=FALSE}
## 5 GBDT model
trainControl_gbm = trainControl(method = "cv",  
                              number = 10)    
gbm_gridca = expand.grid(interaction.depth = 1:8,  
                         n.trees = 1000,          
                         shrinkage = c(0.001, 0.005, 0.01, 0.05, 0.1),
                       n.minobsinnode = 10)
gbm_train = train(medianHouseValue ~ ., data = train_ca,
                   method = "gbm",
                   trControl = trainControl_gbm,
                   tuneGrid = gbm_gridca,
                   verbose = FALSE,
                   metric = "RMSE",  
                   distribution = "gaussian")
gbm_model = gbm(medianHouseValue ~ ., data = train_ca, distribution = "gaussian", 
                n.trees=1000,interaction.depth = 7,shrinkage = 0.01,n.minobsinnode = 10)
```

Then we look at the XGBoost model. By CV we choose the these 3 best parameters: max_depth, subsample and eta. After the cv selection, we use our best parameters to train the model. Since the loop takes really a long time, you can just run thw last command.
```{r, message=FALSE, echo=FALSE, warning=FALSE}
## 6 XGBoost model
xtrain = xgb.DMatrix(data=as.matrix(train_ca[,-9]),label=train_ca[,9])
param_grid = expand.grid(max_depth = c(4, 6, 8, 10),  # Example depths
                          subsample = c(0.5, 0.7, 0.9),  # Example subsample rates
                          eta = c(0.01, 0.05, 0.1),  # Example learning rates
                          stringsAsFactors = FALSE)

# Initialize variables to store the best parameters and lowest RMSE
best_params = list()
min_rmse = Inf

# Loop through the grid
for(i in 1:nrow(param_grid)) {
  params = list(
    booster = "gbtree",
    max_depth = param_grid$max_depth[i],
    subsample = param_grid$subsample[i],
    eta = param_grid$eta[i]
  )
  
  # Perform cross-validation
  cv = xgb.cv(
    params = params,
    data = xtrain,
    nrounds = 10000,
    nfold = 5,  
    watchlist = list(train = xtrain),
    early_stopping_rounds = 10,
    metrics = "rmse",
    maximize = FALSE  
  )
  
  # Check if this model has the lowest RMSE so far
  if(cv$evaluation_log$test_rmse_mean[cv$best_iteration] < min_rmse) {
    min_rmse = cv$evaluation_log$test_rmse_mean[cv$best_iteration]
    best_params = params
  }
}

#print the best params
print(best_params)

xgb_model = xgb.train(params = best_params, data = xtrain,
                       nrounds = 10000, watchlist = list(train = xtrain),
                       early_stopping_rounds = 10)
```


Now we compare the out-of-sample performance for all these 6 models. The plot shows our XGBoost model have the lowest RMSE so we are going to use this model for the following figures.

```{r, message=FALSE, echo=FALSE, warning=FALSE}
## compare out-of-sample performance
xtest = xgb.DMatrix(data=as.matrix(test_ca[,-9]),label=test_ca[,9])

poly_matrix_test = model.matrix(formula,data=test_ca[,-9])
poly_matrix_test = data.frame(scale(poly_matrix_test,center = TRUE, scale=TRUE))
poly_matrix_test[,1] = test_ca[,9]
x_test = as.matrix(poly_matrix_test[,-1])

lm_predict = predict(lm_model,newdata=test_ca[,-9])
lasso_predict = predict(lasso_model,newx = x_test, s = best_lambda)
knn_predict = predict(knn_model,newdata=test_ca[,-9])
rf_predict = predict(rf_model,newdata=test_ca[,-9])
gbm_predict = predict(gbm_model,newdata=test_ca[,-9])
xgb_predict = predict(xgb_model,newdata=xtest)

lm_rmse = rmse(test_ca$medianHouseValue,lm_predict)
lasso_rmse = rmse(test_ca$medianHouseValue,lasso_predict)
knn_rmse = rmse(test_ca$medianHouseValue,knn_predict)
rf_rmse = rmse(test_ca$medianHouseValue,rf_predict)
gbm_rmse = rmse(test_ca$medianHouseValue,gbm_predict)
xgb_rmse = rmse(test_ca$medianHouseValue,xgb_predict)

rmse_value = c(lm_rmse,lasso_rmse,knn_rmse,rf_rmse,gbm_rmse,xgb_rmse)
barplot_heights=barplot(
  rmse_value, 
  names.arg = rep("", length(rmse_value)),
  main = "Root Mean Squared Error (RMSE) Comparison",
  ylab = "RMSE Values",
  ylim = c(0, max(rmse_value) * 1.2),
  cex.names = 0.8, cex.lab = 1.1, cex.main = 1.2)
model_names = c("Linear Regression", "Lasso Regression",
                 "KNN Model", "Random Forest Model",
                 "Gradient Boosting Model", "XGBoost Model")
text(x = barplot_heights, y = -max(rmse_value)*0.1, labels = model_names, 
     srt = 25, adj = 1, xpd = TRUE, cex = 0.8)
text(x = barplot_heights, y = rmse_value + max(rmse_value)*0.05, 
     labels = round(rmse_value, 2), srt = 0, pos = 3, cex = 0.8)




library(ggplot2)
library(maps)

CAhousing_Plot = read.csv('./data/CAhousing.csv')

## data preprocess
origin_plot = CAhousing_Plot
CAhousing_Plot$totalRooms = CAhousing$totalRooms/CAhousing$households
CAhousing_Plot$totalBedrooms = CAhousing$totalBedrooms/CAhousing$households
CAhousing_Plot$medianHouseValue = origin_plot$medianHouseValue

## split the data 
train_index_plot = createDataPartition(CAhousing_Plot$medianHouseValue, p = 0.7, list = FALSE)
train_ca_plot = CAhousing_Plot[train_index, ]
test_ca_plot = CAhousing_Plot[-train_index, ]

# Get California map data
california_map = map_data("state", region = "california")

# Create the plot
ggplot() +
  geom_polygon(data = california_map, aes(x = long, y = lat, group = group), fill = "lightgray", color = "white") + # Draw the base map
  geom_point(data = CAhousing_Plot, aes(x = longitude, y = latitude, color = medianHouseValue), size = 1, alpha = 0.5) +
  scale_color_gradient(low = "yellow", high = "blue") + # Color scale for house values
  labs(title = "Original Median House Value", x = "Longitude", y = "Latitude", color = "Median House Value") +
  theme_minimal()+
  coord_fixed(1.3)

# Step 1: Predict medianHouseValue using the scaled test data
test_ca$predictedMedianHouseValue = predict(lm_model, newdata = test_ca)

# Step 2: Calculate residuals
test_ca$residuals = test_ca$medianHouseValue - test_ca$predictedMedianHouseValue

# Step 3: Merge predictions and residuals with the original unscaled test data

test_ca_plot$predictedMedianHouseValue = test_ca$predictedMedianHouseValue
test_ca_plot$residuals = test_ca$residuals

# Plot the model's predictions
ggplot() +
  geom_polygon(data = california_map, aes(x = long, y = lat, group = group), fill = "lightgray", color = "white") +
  geom_point(data = test_ca_plot, aes(x = longitude, y = latitude, color = predictedMedianHouseValue), size = 1, alpha = 0.5) +
  scale_color_gradient(low = "yellow", high = "blue") +
  labs(title = "Predicted Median House Value", x = "Longitude", y = "Latitude", color = "Predicted Value") +
  theme_minimal() +
  coord_fixed(1.3)

# Plot the model's residuals
ggplot() +
  geom_polygon(data = california_map, aes(x = long, y = lat, group = group), fill = "lightgray", color = "white") +
  geom_point(data = test_ca_plot, aes(x = longitude, y = latitude, color = residuals), size = 1, alpha = 0.5) +
  scale_color_gradient2(low = "red", mid = "white", high = "blue", midpoint = 0) +
  labs(title = "Residuals of Predictions", x = "Longitude", y = "Latitude", color = "Residuals") +
  theme_minimal() +
  coord_fixed(1.3)




```


```{r, message=FALSE, echo=FALSE, warning=FALSE}
#Joe Code.  Lasso tuning

library(gamlr)
library(dplyr)
library(rsample)


Housing = read.csv('./data/CAhousing.csv')

#Include log terms for income, housing value, population and households.  This will standardize data, in practice.  Do we also NEED to standardize data on top of this?

#How are others modifying data?

Housing2 <- Housing %>%
  mutate(
    avgRooms = totalRooms / households,
    avgBedrooms = totalBedrooms / households,
    log_population = log(population),
    log_households = log(households),
    log_medianIncome = log(medianIncome),
    log_medianHouseValue = log(medianHouseValue)
  )

#tract categorical variable... but tracts are already each ind. data point >:/.  Round to single decimal to capture broader location trends

Housing2$latitude_f <- as.factor(round(Housing2$latitude,1))
Housing2$longitude_f <- as.factor(round(Housing2$longitude,1))
Housing2 <- transform(Housing2, tract = interaction(latitude_f, longitude_f, drop = TRUE))

# Perform one-hot encoding
tract_matrix <- model.matrix(~ tract - 1, data = Housing2)

#Combine one-hot encoding to matrix to check results
#Housing2 <- cbind(Housing2, tract_matrix)

#Create df for feature matrix
Housing3 <- Housing2 %>%
   dplyr::select(avgRooms, avgBedrooms, log_population, log_households, log_medianIncome, housingMedianAge)

#Qi code to generate feature matrix
predictor = Housing3[,-1]
variables = names(predictor)
squared_terms = paste0("I(", variables, "^2)", collapse = " + ")
formula = as.formula(paste("~ .^2 +", squared_terms))
predictors =  model.matrix(formula, data = predictor)

#Combine feature matrix and one hot tract variables
lasso_df <- data.frame(predictors, medianHouseValue = Housing2$medianHouseValue)
lasso_features <- cbind(lasso_df, tract_matrix)
lasso_features <- as.data.frame(lasso_features)

#Split into training and testing data
lasso_split = initial_split(lasso_features, prop = 0.8)
lasso_train = training(lasso_split)
lasso_test = testing(lasso_split)

#Prepare training and testing data
x_train = model.matrix(medianHouseValue ~ . - 1, data = lasso_train) # -1 to exclude intercept
y_train = lasso_train$medianHouseValue

x_test = model.matrix(medianHouseValue ~ . - 1, data = lasso_test)
y_test = lasso_test$medianHouseValue

lasso_1 = cv.gamlr(x_train, y_train, nfold=10, verb=TRUE)

lasso1_rsme = sqrt(lasso_1$cvm[lasso_1$seg.1se]) %>% round(4)

#Data exploration.  Important later!
significant_vars = rownames(coef(lasso_1, s = '1se'))[coef(lasso_1, s = '1se')[,1]!= 0]

# #Testing vs other models
# #Just feature matrix w lat/long
# features_2 = cbind(predictors, Housing2$longitude, Housing2$latitude)
# lasso_2 = cv.gamlr(features_2, Housing2$medianHouseValue, nfold=10, verb=TRUE)
# lasso2_rsme = sqrt(lasso_2$cvm[lasso_2$seg.1se]) %>% round(4)
# 
# #Just feature matrix
# features_3 = predictors
# lasso_3 = cv.gamlr(features_3, Housing2$medianHouseValue, nfold=10, verb=TRUE)
# lasso3_rsme = sqrt(lasso_3$cvm[lasso_3$seg.1se]) %>% round(4)
# 
# #Just simple variables w lat/long
# features_4 = Housing2 %>% dplyr::select(avgRooms, avgBedrooms, log_population, log_households, log_medianIncome, housingMedianAge, latitude, longitude)
# lasso_4 = cv.gamlr(features_4, Housing2$medianHouseValue, nfold=10, verb=TRUE)
# lasso4_rsme = sqrt(lasso_4$cvm[lasso_4$seg.1se]) %>% round(4)
# 
# #Just simple variables
# features_5 = Housing2 %>% dplyr::select(avgRooms, avgBedrooms, log_population, log_households, log_medianIncome, housingMedianAge)
# lasso_5 = cv.gamlr(features_5, Housing2$medianHouseValue, nfold=10, verb=TRUE)
# lasso5_rsme = sqrt(lasso_5$cvm[lasso_5$seg.1se]) %>% round(4)

```




