---
title: 'ECO 395M: StateLearning Exercise 4'
author: "Aahil Navroz, Joseph Williams, Qi Suqian"
date: "`r Sys.Date()`"
output: 
  md_document:
---
# ECO 395M: StatLearning Exercise 3

Aahil Navroz, Joseph Williams, Qi Suqian

04/22/2023

## Clustering and PCA

```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(ggcorrplot)
library(tidyverse)
library(ClusterR)

wine = read.csv("./data/wine.csv")
chemical = wine[,-(12:13)]
chemical = scale(chemical, center=TRUE, scale=TRUE)
color_sample = wine[,-12]
color_sample$color = as.numeric(color_sample$color == "red")
quality_sample = wine[,-13]
ggcorrplot(cor(color_sample),lab = TRUE,hc.order = TRUE)
ggcorrplot(cor(quality_sample),lab = TRUE,hc.order = TRUE)
# it seems total.sulfur.dioxide, volatile.acidity and chlorides are the 3 most related variables to color
# alcohol is the most related variable to quality
# summary statistic for red and white
```

## Market segmentation

*Use the data to come up with some interesting, well-supported insights about the audience and give your client some insight as to how they might position their brand to maximally appeal to each market segment.*

To get a basic idea of our data lets start with a two-way correlation plot.

```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(dplyr)

market = read.csv("./data/social_marketing.csv")
features = market %>% select(-X)
ggcorrplot(cor(features),
           lab = TRUE,
           hc.order = TRUE,
           lab_size = .1,
           tl.cex=8
           )      # Vertical alignment

```

Great! So already we're seeing some clusters.  I notice see two way correlation within the following groups: 

* group1_familyvalues: `parenting`, `religion`, `sports_fandom`, `food`, `school`, `family`
* group2_collegeboy: `college_uni`, `online_gaming`, `sports_playing`
* group3_fashionable: `beauty`, `cooking`, `fashion`
* group4_yuppie: `personal_fitness`, `health_nutrition`, `outdoors`
* group5_neoliberal: `politics`, `travel`, `computers`
* group6_socialyte: `shopping`, `chatter`, `photo_sharing`

Lets organize counts to see how many followers had at least two or more tweets in at least 2, (or 3 for family_values and socialyte) variables of each cluster.  Lets also filter out users who are in more than 3 of these respective groups to eliminate generalists and get a better personality portrait of our followers.

```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(ggplot2)

market1 <- market %>% 
  rowwise() %>%
  mutate(
    group1_familyvalues = as.numeric(sum(parenting > 1, religion > 1, sports_fandom > 1, food > 1, school > 1, family > 1) >= 3),
    group2_collegeboy = as.numeric(sum(college_uni > 1, online_gaming > 1, sports_playing > 1) >= 2),
    group3_fashionable = as.numeric(sum(beauty > 1, cooking > 1, fashion > 1) >= 2),
    group4_yuppie = as.numeric(sum(personal_fitness > 1, health_nutrition > 1, outdoors > 1) >= 2),
    group5_neoliberal = as.numeric(sum(politics > 1, travel > 1, computers > 1) >= 2),
    group6_socialyte = as.numeric(sum(shopping > 1, chatter > 1, photo_sharing > 1) >= 3),
    num_groups = sum(group1_familyvalues,group2_collegeboy,group3_fashionable,group4_yuppie,group5_neoliberal,group6_socialyte)
  ) %>%
  ungroup()

group_counts1 <- market1 %>%
  filter(num_groups<4) %>%
  summarise(
    group1_familyvalues = sum(group1_familyvalues, na.rm = TRUE),
    group2_collegeboy = sum(group2_collegeboy, na.rm = TRUE),
    group3_fashionable = sum(group3_fashionable, na.rm = TRUE),
    group4_yuppie = sum(group4_yuppie, na.rm = TRUE),
    group5_neoliberal = sum(group5_neoliberal, na.rm = TRUE),
    group6_socialyte = sum(group6_socialyte, na.rm = TRUE)
  ) %>%
  pivot_longer(
    cols = everything(),
    names_to = "group",
    values_to = "user_count"
  )

# group_counts2 <- market1 %>%
#   filter(num_groups<3) %>%
#   summarise(
#     group1_familyvalues = sum(group1_familyvalues, na.rm = TRUE),
#     group2_collegeboy = sum(group2_collegeboy, na.rm = TRUE),
#     group3_fashionable = sum(group3_fashionable, na.rm = TRUE),
#     group4_yuppie = sum(group4_yuppie, na.rm = TRUE),
#     group5_neoliberal = sum(group5_neoliberal, na.rm = TRUE),
#     group6_socialyte = sum(group6_socialyte, na.rm = TRUE)
#   ) %>%
#   pivot_longer(
#     cols = everything(),
#     names_to = "group",
#     values_to = "user_count"
#   )

# Create the bar plot
ggplot(group_counts1, aes(x = group, y = user_count)) +
  geom_bar(stat = "identity") +
  ggtitle("Counts by Group for Users in Max 3 Groups") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# # Create the bar plot
# ggplot(group_counts2, aes(x = group, y = user_count)) +
#   geom_bar(stat = "identity") +
#   ggtitle("Counts by Group for Users in Max 2 Groups") +
#   ylim(0, 2000) +
#   theme_minimal() +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Over 1500 users are in the loosely constructed `familyvalues', 'yuppie', and 'socialyte' groups, respectively.  Even with filtering efforts, however, many users are likely counted 2 or even 3 times. Before we go farther in this direction, lets shift to machine learning algorithms so that our clusters are definite and exhaustive.  We'll start with K-means and K_means++ clustering with K=5-7 clusters, since we see 6 key groups off the bat.  See below visualizations and counts per cluster.


```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(ClusterR)  # for kmeans++
library(ggrepel)

#for K++
# Extract and round centroids
# centroids <- clust2$centroids
# colnames(centroids) <- colnames(ft)
# clust2_rounded <- round(centroids, 2)
# print(clust2_rounded)

# Data preprocessing
ft = market[,-1]  # Exclude the first column
ft = scale(ft, center=TRUE, scale=TRUE)

# Execute kmeans
set.seed(1)
k_values <- c(5, 7, 6)

for (k in k_values) {
  clust = kmeans(ft, k, nstart=50)
  
  # Extract and round centroids
  centroids <- round(clust$centers, 2)
  
  #return cluster data for k=6
  # if (k==6){
  #   print(centroids)
  # }
    
  
  # Convert centroids to a dataframe
  centroids_df <- as.data.frame(centroids)
  centroids_df <- centroids_df %>%
    rownames_to_column(var = "centroid")
  
  # Convert centroids dataframe to long format for plotting
  centroids_long <- centroids_df %>%
    gather(key = "variable", value = "value", -centroid)
  
  # Plot pie charts for each centroid
  pie_plots <- ggplot(centroids_long, aes(x = "", y = value, fill = variable)) +
    geom_bar(stat = "identity", width = 1) +
    coord_polar("y", start=0) +
    facet_wrap(~centroid, ncol = 3) +
    labs(title = paste("Pie Charts of Centroids, K =", k),
         fill = "Variable") +
    theme_void()
  
  # Add labels for variables with centroid value > 1 without overlap
  label_data <- centroids_long %>%
    filter(value > 1)
  
  pie_plots <- pie_plots +
    geom_text_repel(data = label_data, aes(label = variable, y = value), 
                    position = position_stack(vjust = 0.5), 
                    size = 3) +
    theme(legend.position = "none")
  
  # Count number of users in each cluster
  cluster_counts <- as.data.frame(table(clust$cluster))
  colnames(cluster_counts) <- c("Cluster", "Users")
  
  
  print(pie_plots)
  print(cluster_counts)
  
}




```

We are able to confirm using unsupervised learning pretty much the same clusters we identified using intuition and basic tools. Labeled variables have centroid values over a certain, relatively-high threshold.  Our results use standard K-means but we've verified that results are repeatable with K-means++ start up. Regarding selection of K we notice the following points:

- Each K identifies a 'spam' category where no variables are dominant.
- K=5 splits up our 'fashionable' and 'socialyte' clusters between the four non-spam groups, so that the only major feature of one of the clusters is `photo_sharing`.  
- K=6,7 recovers the 'fashionable' cluster, and some of the 'socialyte' cluster. Its likely that the 'socialyte' characteristics `shopping`, `chatter` and `photo sharing` represent popular uses of Twitter which are more easily distributed between users in other categories.

Altogether, its clear that K=6 weeds out the most spam posts and maintains an even distribution between the other categories, a key requirement of clustering. Most importantly, it aligns incredibly well with our opening analysis. Next, our key insights will rely on K-means clustering using K=6 and identify five distinct non-spam groups.


##### Insights & Recommended Steps
We've identified 5 roughly even-sized market segments.  Here are the two largest in descending order:

- **Health-conscious adults (likely mid-twenties to thirties)**. In my city we call these yoga enthusiasts and REI shoppers 'yuppies', short for 'young professionals'.  This market segment shared twitter engagement in `health_nutrition`, `personal_fitness`, `outdoors`.  This segment also showed notable interest for `eco`, though it did not meet the display threshold.  We recommend an approach that shows sustainability efforts, and connects your product to outdoor engagement and mental health.

- **Traditional Americans**. Don't forget about the heartland, the silent majority, your minivan moms and sports bar dads. This market segment showed over-threshold engagement with more categories than any other group, by far, indicating they are 'classic American' consumers- not part of any niche group.  Key characteristics align with traditional values, and include `family`, `food`, `religion`, `sports fandom`, `school` and `parenting`. To appeal to this group, show that your product could easily find its way to a children's soccer game or family reunion.

Overall, to appeal to both groups and maximize market outreach, perhaps you are the beverage of choice for the modern parent... but not *too* modern. Perhaps its being consumed on a good ol' fashioned camping trip. Don't forget to put ice in the cooler!



```{r, message=FALSE, echo=FALSE, warning=FALSE}


```



## Association rules for grocery purchases


## Image classification with neural networks

In this problem, you will train a neural network to classify satellite images.  In the [data/EuroSAT_RGB](https://github.com/jgscott/STA380/tree/master/data/EuroSAT_RGB) directory, you will find 11 subdirectories, each corresponding to a different class of land or land use: e.g. industrial, crops, rivers, forest, etc.  Within each subdirectory, you will find examples in .jpg format of each type.  (Thus the name of the directory in which the image lives is the class label.)  

Your job is to set up a neural network that can classify the images as accurately as possible.  Use an 80/20 train test split.  Summarize your model and its accuracy in any way you see fit, but make you include _at a minimum_ the following elements:

- overall test-set accuracy, measured however you think is appropriate   
- show some of the example images from the test set, together with your model's predicted classes. 
- a confusion matrix showing the performance of the model on the set test, i.e. a table that cross-tabulates each test set example by (actual class, predicted class).  

I strongly recommend the use of PyTorch in a Jupyter notebook for this problem; look into PyTorch's `ImageFolder` data set class, which will streamline things considerably.  I'll give you the first block of code in my Jupyter notebook, which looks like this. I've handled the resizing and normalization of the images for you -- you can take it from here.


```
# Necessary Imports
import torch
import torchvision
import torchvision.transforms as transforms
from torchvision.datasets import ImageFolder
import matplotlib.pyplot as plt
import numpy as np

# Set the directory where your data is stored
data_dir = '../data/EuroSAT_RGB'

# Set the batch size for training and testing
batch_size = 4

# Define a transformation to apply to the images
transform = transforms.Compose(
    [transforms.Resize((32, 32)),  # Resize images to 32x32
     transforms.ToTensor(),  # Convert image to PyTorch Tensor data type
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])  # Normalize the images

# Load the training data
dataset = ImageFolder(root=data_dir, transform=transform)

# Create data loaders for training and testing datasets
data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Print some samples to verify the data loading
data_iter = iter(data_loader)
images, labels = data_iter.next()
print(images.shape, labels.shape)

# Function to show an image
def imshow(img):
    img = img / 2 + 0.5  # Unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

 # Get some random training images
dataiter = iter(data_loader)
images, labels = dataiter.next()

# Show images
imshow(torchvision.utils.make_grid(images))

# Print labels
print(' '.join('%5s' % dataset.classes[labels[j]] for j in range(batch_size)))

```

One tip: in our example of a convolutional neural network in class, we had black and white images, and therefore _one_ input channel in our 2D convolutions.  These are RGB images here, and so you'll need to modify the first convolutional layer accordingly to handle _three_ input channels.
